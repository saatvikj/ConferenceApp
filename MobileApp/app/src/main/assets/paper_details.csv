title,authors,topics,venue,time,abstract
Large Scale Analogical Reasoning,"Letha Lyvers,Eusebio Brinkman","CS: Conceptual inference and reasoning
CS: Structural learning and knowledge capture
KRR: Qualitative Reasoning","Hall A, Building 1","02/12/2018,10:30-11:30","It has been argued that one can use cognitive simulation of analogical
processing to answer comparison questions.  In the context of a
knowledge base (KB) system, a comparison question takes the form: What
are the similarities and/or differences between A and B?, where
\concept{A} and \concept{B} are concepts in the KB.  Previous attempts
to use a general purpose analogical reasoner to answer this question
revealed three major problems: (a) the system presented too much
information in the answer and the salient similarity or difference was
not highlighted (b) analogical inference found some incorrect
differences (c) some expected similarities were not found. The primary
cause of these problems was the lack of availability of a well-curated
KB, and secondarily, there were also some algorithmic deficiencies.
In this paper, we present an of comparison questions that is inspired
by the general model of analogical reasoning, but is specific to the
questions at hand. We also rely on a well-curated biology KB.  We
present numerous examples of answers produced by the system and
empirical data on the quality of the answers to claim that we have
addressed many of the problems faced in the previous system."
Learning Models of Unknown Events,"Vallie Boissonneault,Alisha Bunge,Gayla Sprague","CM: Symbolic AI
CS: Problem solving and decision making
CS: Introspection and meta-cognition
CS: Structural learning and knowledge capture
PS: Learning Models for Planning and Diagnosis
PS: Plan Execution and Monitoring","Hall B, Building 1","02/12/2018,10:50-13:00","Agents with incomplete models of their environment are likely to be surprised. For agents in immense environments that defy complete modeling, this represents an opportunity to learn. We investigate approaches for situated agents to detect surprises, discriminate among different forms of surprise, and hypothesize new models for the unknown events that surprised them. We instantiate these approaches in a new goal reasoning agent (named FOOLMETWICE), investigate its performance in simulation studies, and show that it produces plans with significantly reduced execution cost when compared to not learning models for surprising events."
Learning Concept Embeddings for Query Expansion by Quantum Entropy Minimization,"Darwin Garlock,James Lex","NLPKR: Natural Language Processing (General/Other)
NLPML: Natural Language Processing (General/Other)
NMLA: Neural Networks/Deep Learning","Hall C, Building 1","02/12/2018,11:30-12:30","In web search, users queries are formulated using only few terms and term-matching retrieval functions could fail at retrieving relevant documents. Given a user query, the technique of query expansion (QE) consists in selecting related terms that could enhance the likelihood of retrieving relevant documents. Selecting such expansion terms is challenging and requires a computational framework capable of encoding complex semantic relationships. In this paper, we propose a novel method for learning, in a supervised way, semantic representations for words and phrases. By embedding queries and documents in special matrices, our model disposes of an increased representational power with respect to existing approaches adopting a vector representation. We show that our model produces high-quality query expansion terms. Our expansion increase IR mesures beyond expansion from current word-embeddings models and well-established traditional QE methods."
Non-Restarting SAT Solvers With Simple Preprocessing Efficiently Simulate Resolution,"Darwin Garlock,Doloris Houge,Kathey Calmes","SCS: Constraint Satisfaction
SCS: Constraint Learning and Acquisition
SCS: SAT and CSP: Evaluation and Analysis
SCS: Satisfiability (General/Other)","Hall D, Building 1","02/12/2018,14:00-15:30","Propositional satisfiability (SAT) solvers based on conflict directed
clause learning (CDCL) implicitly produce resolution refutations of
unsatisfiable formulas. The precise class of formulas for which they
can produce polynomial size refutations has been the subject of
several studies, with special focus on the clause learning aspect of
these solvers. The results, however, either assume the use of
non-standard and non-asserting learning schemes such as FirstNewCut,
or rely on polynomially many restarts for simulating individual steps
of a resolution refutation, or work with a theoretical model that
significantly deviates from certain key aspects of all modern CDCL
solvers such as learning only one asserting clause from each conflict
and other techniques such as conflict guided backjumping and clause
minimization. We study non-restarting CDCL solvers that learn only one
asserting clause per conflict and show that, with simple preprocessing
that depends only on the number of variables of the input formula,
such solvers can polynomially simulate resolution."
Worst-Case Solution Quality Analysis When Not Re-Expanding Nodes in Best-First Search,"Noella Hayslip,Alisha Bunge","HSO: Heuristic Search
HSO: Evaluation and Analysis (Search and Optimization)","Hall E, Building 1","02/12/2018,15:00-16:30","The use of inconsistent heuristics with A* can result in increased runtime due to the need to re-expand nodes. Poor performance can also be seen with Weighted A* if nodes are re-expanded. While the negative impact of re-expansions can often be minimized by setting these algorithms to never expand nodes more than once, the result can be a lower solution quality. In this paper, we formally show that the loss in solution quality can be bounded based on the amount of inconsistency along optimal solution paths. This bound holds regardless of whether the heuristic is admissible or inadmissible, though if the heuristic is admissible the bound can be used to show that not re-expanding nodes can have at most a quadratic impact on the quality of solutions found when using A*. We then show that the bound is tight by describing a process for the construction of graphs for which a best-first search that does not re-expand nodes will find solutions whose quality is arbitrarily close to that given by the bound. Finally, we will use the bound to extend a known result regarding the solution quality of WA* when weighting a consistent heuristic, so that it applies to other types of heuristic weighting."
Natural Temporal Difference Learning,"Katharina Schrupp,Noella Hayslip",,"Hall A, Building 2","03/12/2018,10:30-11:30","In this paper we investigate the application of natural gradient descent to Bellman error based reinforcement learning algorithms. This combination is interesting because natural gradient descent is invariant to the parameterization of the value function. This invariance property means that natural gradient descent adapts its update directions to correct for poorly conditioned representations. We present and analyze quadratic and linear time natural temporal difference learning algorithms, and prove that they are covariant. We conclude with experiments which suggest that the natural algorithms can match or outperform their non-natural counterparts using linear function approximation, and drastically improve upon their non-natural counterparts when using non-linear function approximation."
Relaxation Search: a Simple Way of Managing Optional Clauses,"Noella Hayslip,Roselyn Picklesimer,Youlanda Griffie","SCS: Constraint Optimization
SCS: SAT and CSP: Solvers and Tools
SCS: Satisfiability (General/Other)","Hall B, Building 2","03/12/2018,10:50-13:00","A number of problems involve managing a set of optional clauses. For example, the soft clauses in a MaxSat formula are optional---they can be falsified for a cost. Similarly when computing a Minimum Correction Set for an unsatisfiable formula all clauses are optional---some can be falsified in order to make the
  remaining satisfiable. In both of these cases the task is to find a subset of the optional clauses that achieves some optimization criteria and is satisfiable. Relaxation search is a simple method of using a standard SAT solver to solve this task. Relaxation search is very easy to implement, sometimes requiring only a simple   modification of the variable selection heuristic in the SAT solver. Furthermore, considerable flexibility and control can be achieved over the order in which subsets of optional clauses examined. We demonstrate how relaxation search can be used to solve MaxSat and to compute Minimum Correction Sets. In both cases  relaxation search is able to achieve state-of-the-art performance and solve some instances other solvers are not able to solve."
Using Response Functions to Measure Strategy Strength,Ericka Cowger,"GTEP: Game Theory
GTEP: Equilibrium
GTEP: Imperfect Information","Hall C, Building 2","03/12/2018,11:30-12:30","Extensive-form games are a powerful tool for representing complex multi-agent interactions. Nash equilibrium strategies are commonly used as a solution concept for extensive-form games, but many games are too large for the computation of Nash equilibria to be tractable. In these large games, exploitability has traditionally been used to measure deviation from Nash equilibrium, and thus strategies are aimed to achieve minimal exploitability. However, while exploitability measures a strategy's worst-case performance, it fails to capture how likely that worst-case is to be observed in practice. In fact, empirical evidence has shown that a less exploitable strategy can perform worse than a more exploitable strategy in one-on-one play against a variety of opponents. In this work, we propose a class of response functions that can be used to measure the strength of a strategy. We prove that standard no-regret algorithms can be used to learn optimal strategies for a scenario where the opponent uses one of these response functions. We demonstrate the effectiveness of this technique in Leduc poker against opponents that use the UCT Monte Carlo tree search algorithm."
Optimal and Efficient Stochastic Motion Planning in Partially-Known Environments,"Glady Mahan,Doloris Houge,Vallie Boissonneault","PS: Mixed Discrete/Continuous Planning
RU: Uncertainty in AI (General/Other)
ROB: Motion and Path Planning
ROB: Robotics (General/Other)","Hall D, Building 2","03/12/2018,14:00-15:30","A framework capable of computing optimal control policies for a continuous system in the presence of both action and environment uncertainty is presented in this work.  The framework decomposes the planning problem into two stages: an offline phase that reasons only over action uncertainty and an online phase that quickly reacts to the uncertain environment.  Offline, a bounded-parameter Markov decision process (BMDP) is employed to model the evolution of the stochastic system over a discretization of the environment.  Online, an optimal control policy over the BMDP is computed.  Upon the discovery of an unknown environment feature during policy execution, the BMDP is updated and the optimal control policy is efficiently recomputed.  Depending on the desired quality of the control policy, a suite of methods is presented to incorporate new information into the BMDP with varying degrees of detail online.  Experiments confirm that the framework recomputes high-quality policies in seconds and is orders of magnitude faster than existing methods."
Learning Scripts as Hidden Markov Models,"Letha Lyvers,Eusebio Brinkman","NLPML: Natural Language Processing (General/Other)
NMLA: Graphical Model Learning","Hall E, Building 2","03/12/2018,14:00-15:30","Scripts have been proposed to model the stereotypical event sequences found in narratives. They can be applied to make a variety of inferences including filling gaps in the narratives and resolving ambiguous references. This paper proposes the first formal framework for scripts based on Hidden Markov Models (HMMs). Our framework supports robust inference and learning algorithms, which are lacking in previous clustering models. We develop an algorithm for structure and parameter learning based on Expectation Maximization and evaluate it on a number of natural and synthetic datasets. The results show that our algorithm is superior to several informed baselines for predicting future events given some past history."
Mapping Users Across Networks by Manifold Alignment on Hypergraph,"Vallie Boissonneault,Alisha Bunge,Gayla Sprague","AIW: Machine learning and the web
AIW: Ontologies and the web: creation, extraction, evolution, mapping, merging, and alignment; tags and folksonomies
AIW: Social networking and community identification","Hall A, Building 3","04/12/2018,10:30-11:30","Nowadays many people are members of multiple online social networks simultaneously, such as Facebook, Twitter and some other instant messaging circles. But these networks are usually isolated from each other. Mapping common users cross these social networks will be beneficial for cross network recommendation or expanding oneâ€™s social circle. Methods based on username comparison perform well on parts of users, however they can not work in the following situations: (a) users choose completely different usernames in different networks; (b) a unique username corresponds to different individuals. In this paper, we propose to utilize social structures to improve the mapping performance. Specifically, a novel subspace learning algorithm, Manifold Alignment on Hypergraph (MAH), is proposed. Different from traditional semi-supervised manifold alignment methods, we use hypergraph to model high-order relations here. For a target user in one network, the proposed algorithm ranks all users in the other network by their probabilities of being the corresponding user. Moreover, methods based on username comparison can be incorporated with our algorithm easily to further boost the mapping accuracy. In experiments, we use both simulation data and real world data to test the proposed method. Experiment results have demonstrated the effectiveness of our proposed algorithm in mapping users cross networks."
Compact Aspect Embedding For Diversified Query Expansion,"Darwin Garlock,James Lex",AIW: Enhancing web search and information retrieval,"Hall B, Building 3","04/12/2018,10:50-13:00","Diversified query expansion (DQE) based approaches aim to select a set of expansion terms with less redundancy among them while covering  as many query aspects as possible. Recently they have experimentally demonstrate their effectiveness for the task of search result diversification. One challenge faced by existing DQE approaches is how to  ensure the aspect coverage. In this paper, we propose a novel method for DQE, called compact aspect embedding, which exploits trace norm regularization to  learn a  low rank vector space  for the query, with each eigenvector of the learnt vector space representing an aspect, and the absolute value of its corresponding eigenvalue representing the association strength of that aspect to the query.  Meanwhile, each expansion term is mapped into the vector space as well. Based on this novel representation of the query aspects and expansion terms, we design a greedy selection strategy to choose a set of expansion terms to explicitly cover all possible aspects of the query. We test our method  on several TREC diversification data sets, and show our method significantly outperforms the state-of-the-art approaches."
Contraction and Revision over DL-Lite TBoxes,"Darwin Garlock,Doloris Houge,Kathey Calmes","KRR: Belief Change
KRR: Description Logics
KRR: Nonmonotonic Reasoning","Hall C, Building 3","04/12/2018,11:30-12:30","An essential task in managing DL ontologies is to deal with changes over the ontologies. 
In particular, outdated axioms have to be removed from the ontology 
and newly formed axioms have to be incorporated into the ontology.
Such changes are formalised as the operations of contraction and revision in the literatures. 
The operations can be defined in various ways.
To investigate properties of a defined operation, it is best to identify some postulates that completely 
characterise the operation such that on the one hand the operation satisfies the postulates 
and on the other hand it is the only operation that satisfies all the postulates.
Such characterisation results have never been shown for contractions under DLs.
In this paper, we define model-based contraction and revision for DL-Lite$_{core}$ TBoxes
and provide characterisation results for both operations.
As a first step for applying the operations in practice, 
we also provide tractable algorithms for both operations.
Since DL semantics incurs infinite numbers of models for DL-Lite TBoxes,
it is not feasible to develop algorithms involving DL models.
The key to our operations and algorithms is the development of an alternative semantics called type semantics. Type semantics closely resembles the semantics underlays propositional logic,
thus it is more succinct than DL semantics. Most importantly, given a finite signature, any DL-Lite$_{core}$ TBox has finite numbers of type models."
Zero Pronoun Resolution as Ranking,"Noella Hayslip,Alisha Bunge",NLPTM: Evaluation and Analysis,"Hall D, Building 3","04/12/2018,14:00-15:30","Compared to overt pronoun resolution, there is less work on the more challenging task of zero pronoun resolution. State-of-the-art approaches to zero pronoun resolution are supervised, requiring the availability of documents containing manually resolved zero pronouns. In contrast, we propose in this paper an unsupervised approach to this task. Underlying our approach is the novel idea of employing a model trained on manually resolved overt pronouns to resolve zero pronouns. Experimental results on the OntoNotes corpus are encouraging: our unsupervised model rivals its supervised counterparts in performance."
Supervised Transfer Sparse Coding,"Katharina Schrupp,Noella Hayslip","NMLA: Classification
NMLA: Transfer, Adaptation, Multitask Learning
NMLA: Supervised Learning (Other)","Hall E, Building 3","04/12/2018,14:00-15:30","A combination of sparse coding and transfer learning techniques was shown to be accurate and robust in classification tasks where training and testing objects have a shared feature space but are sampled from different underlying distributions, i.e., belong to different domains. The key assumption in such case is that in spite of the domain disparity, samples from different domains share some common hidden factors. Previous methods often assumed that all the objects in the target domain are not labeled, and thus the training set solely comprised objects from the source domain. However, in real world applications, the target domain often has some labeled objects, or one can always manually label a small number of them. In this paper, we explore such possibility and show how a little amount of labeled data in the target domain can significantly leverage classification accuracy of the state-of-the-art transfer sparse coding methods. We further propose a unified framework named Supervised Transfer Sparse Coding (STSC) which simultaneously optimizes sparse representation, domain transfer and supervised classification. Experimental results on three applications demonstrate that little manual labeling and then learning the model in a supervised fashion can significantly improve classification accuracy."